{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wholesale Customer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domain** This is a canonical dataset taken from the UCI Machine Learning Repository. \n",
    "\n",
    "It is data collected from the sale of products by a grocery wholesaler in Portugal. The values are in \"monetary units\".\n",
    "\n",
    "**Problem** While this task includes labels, it is typically used for clustering or unsupervised learning work. Here we will keep the labels for comparison but will be exploring this dataset in the context of EDA and unsupervised learning in an EDA setting. \n",
    "\n",
    "Two columns will work as label for this set `region` and `channel`.\n",
    "\n",
    "**Solution** We will be working toward a customer segmentation generated by a cluster analysis.\n",
    "\n",
    "**Data** The following analysis shows:\n",
    "\n",
    "- there are 440 rows and 8 useful variable columns in the dataset. Two of these columns are target features `Region` and `Channel`. \n",
    "\n",
    "- there are six integer value columns:\n",
    "   - `Fresh`\n",
    "   - `Milk`\n",
    "   - `Grocery`\n",
    "   - `Frozen`\n",
    "   - `Detergents_Paper`\n",
    "   - `Delicatessen`\n",
    "\n",
    "\n",
    "**Benchmark** As this is an EDA and Unsupervised Learning task, we will not define an explicit benchmark. \n",
    "\n",
    "**Metrics** We will not define a metric for this project at this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments\n",
    "\n",
    "> In mathematics, a moment is a specific quantitative measure, used in both mechanics and statistics, of the shape of a set of points.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Moment_(mathematics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the points represent mass:\n",
    "- the zeroth moment is the total mass\n",
    "- the first moment divided by the total mass is the center of mass\n",
    "- the second moment is the rotational inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the points represent probability density:\n",
    "\n",
    "- the zeroth moment is the total probability (i.e. one)\n",
    "- the first moment is the mean\n",
    "- the second central moment is the variance\n",
    "- the third central moment is the skewness\n",
    "- the fourth central moment (with normalization and shift) is the kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Probability\n",
    "\n",
    "The $k$-th moment of a real-valued continuous function, a **probability density function**, $f(x)$ of a real variable about a value $c$ is\n",
    "\n",
    "$$\\mu_k = \\int_{-\\infty}^\\infty (x - c)^k\\,f(x)\\,\\mathrm{d}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Probabilty\n",
    "\n",
    "The $k$-th moment of a real-valued discrete function, a **probability mass function**,  $p(x)$ of a real variable about a value $c$ is\n",
    "\n",
    "$$\\mu_k = \\sum (x - c)^kp(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Case: the Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the first moment for a discrete valued probabilty where each value has an equal chance of being observed.\n",
    "\n",
    "Then, for a list of $n$ values\n",
    "\n",
    "$$\\mathcal{D} = \\{x_1, \\dots, x_n\\}$$\n",
    "\n",
    "each with probabilty $p(x_k)= \\frac{1}{n}$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\mu_1 = \\sum (x)^1p(x)= \\sum x\\frac{1}{n} = \\frac{1}{n}\\sum x$$\n",
    "\n",
    "This is the well-known mean you are used to. We typically refer to it simply as $\\mu$\n",
    "\n",
    "$$\\mu = \\frac{1}{n}\\sum x$$\n",
    "\n",
    "We also call this an **Expected Value**, $\\mathbb{E}[x]$, and for the common case\n",
    "\n",
    "$$\\operatorname{E}[x] = \\mu$$\n",
    "\n",
    "In python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([2, 11,  7, 19, 14,  9,  9,  4, 16, 12, 16, 18, 13, 16, 10, 13,  1, 6, 12,  2, 14, 14, 1, 1])\n",
    "n = len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(D, bins=20, kde=False)\n",
    "plt.xticks(list(range(1,20)))\n",
    "plt.yticks(list(range(5)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1/(n)*np.sum(D)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(D, bins=20, kde=False)\n",
    "plt.axvline(D.mean())\n",
    "plt.xticks(list(range(1,20)))\n",
    "plt.yticks(list(range(5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Case: the Variance\n",
    "\n",
    "Consider the second **central** moment for a discrete valued probabilty where each value has an equal chance of being observed. We call it **central** because we will center this value around the mean.\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\mu_2 = \\sum (x - \\mu)^2p(x)= \\frac{1}{n}\\sum (x - \\mu)^2$$\n",
    "\n",
    "This is just the expected value of $(x-\\mu)^2$. We call this the **variance**, denoted $\\sigma^2$.\n",
    "\n",
    "$$\\sigma^2 = \\mathbb{E}\\left[(x-\\mu)^2\\right]$$\n",
    "\n",
    "In python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 1/n*np.sum((D-mu)**2)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the square root of the variance is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(D, bins=20, kde=False)\n",
    "plt.axvline(D.mean()+D.std(), color='black', lw=.75, ls=\"dashed\")\n",
    "plt.axvline(D.mean()-D.std(), color='black', lw=.75, ls=\"dashed\")\n",
    "plt.axvline(D.mean()+2*D.std(), color='black', lw=.25, ls=\"dashed\")\n",
    "plt.axvline(D.mean()-2*D.std(), color='black', lw=.25, ls=\"dashed\")\n",
    "plt.axvline(D.mean())\n",
    "plt.xticks(list(range(1,20)))\n",
    "plt.yticks(list(range(5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Cases: Skew and Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness\n",
    "\n",
    "In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Skewness\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Negative_and_positive_skew_diagrams_%28English%29.svg/892px-Negative_and_positive_skew_diagrams_%28English%29.svg.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness is the third **standardized** moment. \n",
    "\n",
    "$$\\gamma = \\operatorname{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3 \\right]$$\n",
    "\n",
    "In python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = 1/n*np.sum(((D-mu)/np.sqrt(var))**3)\n",
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.skew(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(D, bins=20, kde=True)\n",
    "plt.axvline(D.mean()+D.std(), color='black', lw=.75, ls=\"dashed\")\n",
    "plt.axvline(D.mean()-D.std(), color='black', lw=.75, ls=\"dashed\")\n",
    "plt.axvline(D.mean()+2*D.std(), color='black', lw=.25, ls=\"dashed\")\n",
    "plt.axvline(D.mean()-2*D.std(), color='black', lw=.25, ls=\"dashed\")\n",
    "plt.axvline(D.mean())\n",
    "plt.axvline(np.median(D), color=\"red\")\n",
    "plt.xlim(-1,20)\n",
    "plt.xticks(list(range(1,20)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.distributions.extras import pdf_mvsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "mvsk = [0,1,-1,0]\n",
    "pdffunc = pdf_mvsk(mvsk)\n",
    "rng = np.arange(-3, 3, 0.1)\n",
    "ax[0].plot(rng, pdffunc(rng))\n",
    "ax[0].axvline(1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[0].axvline(-1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[0].axvline(2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[0].axvline(-2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[0].axvline(0)\n",
    "ax[0].set_xlim(-3,3)\n",
    "ax[0].set_ylim(-0.3, 1)\n",
    "ax[0].set_title(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(*mvsk))\n",
    "\n",
    "mvsk = [0,1,1,0]\n",
    "pdffunc = pdf_mvsk(mvsk)\n",
    "ax[1].plot(rng, pdffunc(rng))\n",
    "ax[1].axvline(1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[1].axvline(-1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[1].axvline(2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[1].axvline(-2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[1].axvline(0)\n",
    "ax[1].set_xlim(-3,3)\n",
    "ax[1].set_ylim(-0.3, 1)\n",
    "ax[1].set_title(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(*mvsk));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probability theory and statistics, kurtosis (from Greek: κυρτός, kyrtos or kurtos, meaning \"curved, arching\") is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. In a similar way to the concept of skewness, kurtosis is a descriptor of the shape of a probability distribution and, just as for skewness, there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis is the fourth **standardized** moment. \n",
    "\n",
    "$$\\gamma = \\operatorname{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^4 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "mvsk = [0,1,0,0]\n",
    "pdffunc = pdf_mvsk(mvsk)\n",
    "rng = np.arange(-3, 3, 0.1)\n",
    "ax[0].plot(rng, pdffunc(rng))\n",
    "ax[0].axvline(1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[0].axvline(-1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[0].axvline(2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[0].axvline(-2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[0].axvline(0)\n",
    "ax[0].set_xlim(-3,3)\n",
    "ax[0].set_ylim(-0.3, 1)\n",
    "ax[0].set_title(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(*mvsk))\n",
    "\n",
    "mvsk = [0,1,0,4]\n",
    "pdffunc = pdf_mvsk(mvsk)\n",
    "ax[1].plot(rng, pdffunc(rng))\n",
    "ax[1].axvline(1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[1].axvline(-1, color='black', lw=.75, ls=\"dashed\")\n",
    "ax[1].axvline(2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[1].axvline(-2, color='black', lw=.25, ls=\"dashed\")\n",
    "ax[1].axvline(0)\n",
    "ax[1].set_xlim(-3,3)\n",
    "ax[1].set_ylim(-0.3, 1)\n",
    "ax[1].set_title(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(*mvsk));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurt = 1/n*np.sum(((D-mu)/np.sqrt(var))**4)\n",
    "kurt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis is the average (or expected value) of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean, where the \"peak\" would be), contribute virtually nothing to kurtosis, since raising a number that is less than 1 to the fourth power makes it closer to zero. The only data values (observed or observable) that contribute to kurtosis in any meaningful way are those outside the region of the peak; i.e., the outliers. Therefore, kurtosis measures outliers only; it measures nothing about the \"peak.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.kurtosis(D, fisher=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these four values, we can describe the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "D = np.random.randint(1,21,n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "D = np.random.randint(1,21,n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20000\n",
    "D = np.random.randint(1,21,n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200000\n",
    "D = np.random.randint(1,21,n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000000\n",
    "D = np.random.randint(1,21,n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `randint` draws from a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    np.random.randint()\n",
    "\n",
    "    Return random integers from the \"discrete uniform\" distribution of\n",
    "    the specified dtype in the \"half-open\" interval [`low`, `high`). If\n",
    "    `high` is None (the default), then results are from [0, `low`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about `randn`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "D = np.random.randn(n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "D = np.random.randn(n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "D = np.random.randn(n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20000\n",
    "D = np.random.randn(n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200000\n",
    "D = np.random.randn(n)\n",
    "print(\"mu: {}, var: {}, skew: {}, kurt: {}\".format(D.mean(), D.var(), st.skew(D), st.kurtosis(D, fisher=False)))\n",
    "sns.distplot(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers <- read.table('data/Wholesale_customers_data.csv', sep=\",\", header = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(customers); str(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers$Channel = factor(customers$Channel)\n",
    "customers$Region = factor(customers$Region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sum = summary(customers)\n",
    "cust_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(customers$Channel, customers$Region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = Filter(is.numeric, customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(repr)\n",
    "options(repr.plot.width=20, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals = data.frame(feature=colnames(customer_features))\n",
    "sum_vals['mean_'] = sapply(customer_features, mean)\n",
    "sum_vals['median_'] = sapply(customer_features, median)\n",
    "sum_vals['sd_'] = sapply(customer_features, sd)\n",
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt(sum_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "ggplot(melt(sum_vals), aes(x = feature, y = value, fill = variable)) + \n",
    "    geom_bar(stat = \"identity\", width=0.5, position = \"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Survey Monkey [Sample Size Calculator](https://www.surveymonkey.com/mp/sample-size-calculator/) to consider what sample sizes we will need for our data to representative of the whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1 - `n = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals = data.frame(feature=colnames(customer_features))\n",
    "sum_vals['mean_'] = sapply(customer_features, mean)\n",
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr, warn.conflicts = FALSE)\n",
    "samples = list()\n",
    "for (i in 1:10) {\n",
    "    samples[[i]] = sample_n(customer_features, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:10) {\n",
    "    sum_vals[paste('mean_', i)] = sapply(samples[[i]], mean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(melt(sum_vals), aes(x = feature, y = value, fill = variable)) + \n",
    "    geom_bar(stat = \"identity\", width=0.5, position = \"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 - `n = 50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals = data.frame(feature=colnames(customer_features))\n",
    "sum_vals['mean_'] = sapply(customer_features, mean)\n",
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list()\n",
    "for (i in 1:10) {\n",
    "    samples[[i]] = sample_n(customer_features, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:10) {\n",
    "    sum_vals[paste('mean_', i)] = sapply(samples[[i]], mean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(melt(sum_vals), aes(x = feature, y = value, fill = variable)) + \n",
    "    geom_bar(stat = \"identity\", width=0.5, position = \"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2 - `n = 150`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals = data.frame(feature=colnames(customer_features))\n",
    "sum_vals['mean_'] = sapply(customer_features, mean)\n",
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list()\n",
    "for (i in 1:10) {\n",
    "    samples[[i]] = sample_n(customer_features, 150)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in 1:10) {\n",
    "    sum_vals[paste('mean_', i)] = sapply(samples[[i]], mean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(melt(sum_vals), aes(x = feature, y = value, fill = variable)) + \n",
    "    geom_bar(stat = \"identity\", width=0.5, position = \"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('data/Wholesale_customers_data.csv')\n",
    "customers.Region = customers.Region.astype('category')\n",
    "customers.Channel = customers.Channel.astype('category')\n",
    "customer_features = customers.select_dtypes([int])\n",
    "\n",
    "display(customers.info())\n",
    "display(customers.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review-statistics-parameters'></a>\n",
    "\n",
    "#### Review: Sample Statistics and Parameters\n",
    "\n",
    "---\n",
    "\n",
    "Recall that we use sample statistics to estimate population parameters. Our goal is to calculate sample statistics and then rely on properties of a random sample (and perhaps additional assumptions) to make inferences that we can generalize to the larger population of interest.\n",
    "\n",
    "Below is a table comparing some example sample statistics and population parameters:\n",
    "\n",
    "Metric  | Statistic  | Parameter \n",
    "-------- | ---------- | -------- \n",
    "mean   | $$\\bar{x} = \\frac{\\sum x}{n}$$ | $$ \\mu = \\frac{\\sum x}{N} $$      \n",
    "standard deviation   | $$ s = \\sqrt{\\frac{\\sum_i (x_i - \\bar{x})^2}{n-1}} $$ | $$ \\sigma = \\sqrt{\\frac{\\sum_i (x_i - \\mu)^2}{N} } $$\n",
    "correlation   | $$ r = \\frac{\\hat{Cov}(X, Y)}{s_X s_Y} $$ | $$ \\rho = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Distribution\n",
    "\n",
    "---\n",
    "\n",
    "The normal distribution is arguably the most commonly used distribution in all of statistics. **Normality** is an assumption that underlies many statistical tests and serves as a convenient model for the distribution of many (but not all!) variables.\n",
    "\n",
    "The normal distribution relies on two parameters: \n",
    "- The population mean\n",
    "- The population standard deviation \n",
    "\n",
    "If a variable follows a Normal distribution exactly, its mean, median, and mode will all be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for i in range(1,5):\n",
    "    yy = np.random.normal(size=10**i)\n",
    "    fig.add_subplot(1,4,i)\n",
    "    sns.distplot(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='zdist-rule'></a>\n",
    "\n",
    "#### The 68-95-99.7 Rule\n",
    "\n",
    "---\n",
    "\n",
    "It is often beneficial to identify how extreme (or far away from the expected value) a particular observation is within the context of a distribution. \n",
    "\n",
    "It is possible to show that, for a Normal distribution:\n",
    "- 68% of observations from a population will fall within $\\pm 1$ standard deviation of the population mean.\n",
    "- 95% of observations from a population will fall within $\\pm 2$ standard deviations of the population mean.\n",
    "- 99.7% of observations from a population will fall within $\\pm 3$ standard deviations of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a visual representation of the 68-95-99.7 rule on the Delicatessen distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(customers.Delicatessen)\n",
    "plt.axvline(customers.Delicatessen.mean(), color='black', lw=3)\n",
    "plt.axvline(customers.Delicatessen.median(), color='red', lw=3)\n",
    "plt.axvline((customers.Delicatessen.mean() - customers.Delicatessen.std()),\n",
    "            color='black', lw=2, ls=\"dashed\")\n",
    "plt.axvline((customers.Delicatessen.mean() + customers.Delicatessen.std()),\n",
    "            color='black', lw=2, ls=\"dashed\")\n",
    "plt.axvline((customers.Delicatessen.mean() + 2*customers.Delicatessen.std()),\n",
    "            color='black', lw=1, ls=\"dashed\")\n",
    "plt.axvline((customers.Delicatessen.mean() - 2*customers.Delicatessen.std()),\n",
    "            color='black', lw=1, ls=\"dashed\")\n",
    "plt.axvline((customers.Delicatessen.mean() + 3*customers.Delicatessen.std()),\n",
    "            color='black', lw=.5, ls=\"dashed\")\n",
    "plt.axvline((customers.Delicatessen.mean() - 3*customers.Delicatessen.std()),\n",
    "            color='black', lw=.5, ls=\"dashed\")\n",
    "plt.axvline(0)\n",
    "plt.xlim(-5000,20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: z-score\n",
    "\n",
    "\n",
    "The z-score of an observation quantifies how many standard deviations the observation is away from the population mean:\n",
    "\n",
    "#### $$ z_i = \\frac{x_i - \\text{population mean of x}}{\\text{standard deviation of x}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_feature_z_scores = (customer_features - customer_features.mean())/customer_features.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = customer_feature_z_scores.sample(4)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot(kind='bar', figsize=(20,5))\n",
    "labels = [\"Sample {}\".format(i) for i in sample.index]\n",
    "plt.xticks(range(sample.shape[0]+2),labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "---\n",
    "\n",
    "Normality underlies many of the inferential techniques that we use in data science.\n",
    "\n",
    "Consider the random variable $X$. We can take a sample from this population of size $n$ and find the mean of that sample. Let's call this sample mean $x_1$. We can take another sample from this population, also of size $n$, and find the mean of that sample. Let's call this sample mean $x_1$. We can do this over and over until we've calculated the mean of every possible sample of size $n$. If we plotted every sample mean on a histogram, we get another distribution called \"the sampling distribution of $\\bar{X}$.\"\n",
    "\n",
    "**This distribution, the sampling distribution of $\\bar{X}$, is Normally distributed even if the distribution of $X$ is not.** (That is, unless some rare conditions are violated).\n",
    "\n",
    "We can formally define [the central limit theorm](http://homepages.math.uic.edu/~bpower6/stat101/Sampling%20Distributions.pdf) like so:\n",
    "\n",
    "> In probability theory, the central limit theorem states that, when independent random variables are added, their sum tends toward a normal distribution (commonly known as a bell curve), even if the original variables themselves are not normally distributed. In more precise terms, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables — each with a well-defined (finite) expected value and finite variance — will be approximately normally distributed, regardless of the underlying distribution.\n",
    "\n",
    "Some properties that arise from the central limit theorem include:\n",
    "\n",
    "> If $X ~ N(\\mu,\\sigma)$, then $\\bar{X}$ is exactly $N(\\mu,\\frac{\\sigma}{\\sqrt{n}})$\n",
    "\n",
    "> If $X$ is not normally distributed, then $\\bar{X}$ is approximately $N(\\mu,\\frac{\\sigma}{\\sqrt{n}})$ if the sample size $n$ is at least 30. As $n$ increases, $\\bar{X}$ becomes asymptotically normally distributed.\n",
    "\n",
    "> If $\\bar{X}$ is normally distributed, then we can use inferential methods that rely on our sample mean, $\\bar{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "#### Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "http://blog.vctr.me/posts/central-limit-theorem.html\n",
    "\n",
    "http://www.usablestats.com/lessons/central_limit\n",
    "\n",
    "http://blog.minitab.com/blog/michelle-paret/explaining-the-central-limit-theorem-with-bunnies-and-dragons-v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few notebooks, we are going to do some Unsupervised Exploration of the `customer` table in our Database.\n",
    "\n",
    "> What does a data scientist do? PCA on the `customer` table. - Joshua Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(customer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.sample(range(10), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = customer_features.describe().T\n",
    "stats['skew'] = skew(customer_features)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Dataset \n",
    "\n",
    "In this notebook, we begin to explore the `customer` table by sampling the table. First, let's sample three random points and examine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = customer_features.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling for a Statistical Description\n",
    "\n",
    "We are able to take the mean and standard deviation of the data, but what if we want to visualize it? \n",
    "\n",
    "Of course, this dataset is small, but we might want techniques that work even when the dataset is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at 1% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_1 = customer_features.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does this compare to the actual mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_1.mean() - stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about this in terms of percent error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_1pct_1.mean() - stats['mean'])/stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_2 = customer_features.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1pct_2.mean() - stats['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_1pct_2.mean() - stats['mean'])/stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeatedly Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = []\n",
    "for _ in range(10):\n",
    "    sample_means.append(customer_features.sample(5).mean())\n",
    "\n",
    "sample_means = np.array(sample_means)\n",
    "(sample_means.mean(axis=0)-stats['mean'])/stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = []\n",
    "for _ in range(50):\n",
    "    sample_means.append(customer_features.sample(5).mean())\n",
    "\n",
    "sample_means = np.array(sample_means)\n",
    "(sample_means.mean(axis=0)-stats['mean'])/stats['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_means = []\n",
    "for _ in range(100):\n",
    "    sample_means.append(customer_features.sample(5).mean())\n",
    "\n",
    "sample_means = np.array(sample_means)\n",
    "(sample_means.mean(axis=0)-stats['mean'])/stats['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do we notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a larger sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally different. Which makes sense ... we're only taking 1% of the data!\n",
    "\n",
    "What if we take a sample of 10% of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_10pct_1 = customer_features.sample(44)\n",
    "(sample_10pct_1.mean() - stats['mean'])/stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this sample good enough for plotting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/2541/is-there-a-reference-that-suggest-using-30-as-a-large-enough-sample-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sample_10pct_1, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Redundancy\n",
    "\n",
    "I claim that there is correlation and redundancy in the `customer` table. What I mean by this is that some features are linear combinations of other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine redundancy by dropping a feature and seeing if the other features can predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_2_for_feature(data,feature):\n",
    "    new_data = data.drop(feature, axis=1)\n",
    "\n",
    "    X_train, \\\n",
    "    X_test,  \\\n",
    "    y_train, \\\n",
    "    y_test = train_test_split(\n",
    "        new_data,data[feature],test_size=0.25\n",
    "    )\n",
    "\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(X_train,y_train)\n",
    "\n",
    "    score = regressor.score(X_test,y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_r_2_for_feature(customer_features,'Detergents_Paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", calculate_r_2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Degergents_paper: \", calculate_r_2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", calculate_r_2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", calculate_r_2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", calculate_r_2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", calculate_r_2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is subject to randomness. There is randomness in my `train_test_split`. Let's do the whole thing many times and take the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_r2_for_feature(data, feature):\n",
    "    scores = []\n",
    "    for _ in range(100):\n",
    "        scores.append(calculate_r_2_for_feature(data, feature))\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", mean_r2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Detergents_Paper: \", mean_r2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", mean_r2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", mean_r2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", mean_r2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", mean_r2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:24} {}\".format(\"Delicatessen: \", mean_r2_for_feature(customer_features,'Delicatessen')))\n",
    "print(\"{:24} {}\".format(\"Detergents_Paper: \", mean_r2_for_feature(customer_features,'Detergents_Paper')))\n",
    "print(\"{:24} {}\".format(\"Fresh: \", mean_r2_for_feature(customer_features,'Fresh')))\n",
    "print(\"{:24} {}\".format(\"Frozen: \", mean_r2_for_feature(customer_features,'Frozen')))\n",
    "print(\"{:24} {}\".format(\"Grocery: \", mean_r2_for_feature(customer_features,'Grocery')))\n",
    "print(\"{:24} {}\".format(\"Milk: \", mean_r2_for_feature(customer_features,'Milk')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "What does this tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the correlation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "sns.pairplot(customer_features, kind='reg')\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = customer_features.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, 0)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True, annot=True,\n",
    "                     cmap='RdBu', fmt='+.3f')\n",
    "    plt.xticks(rotation=45, ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_stats = sample_1pct_1.describe().T\n",
    "samp_stats['skew'] = st.skew(sample_1pct_1)\n",
    "samp_stats['kurt'] = st.kurtosis(sample_1pct_1)\n",
    "samp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = customer_features.describe().T\n",
    "stats['skew'] = st.skew(customer_features)\n",
    "stats['kurt'] = st.kurtosis(customer_features)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANY OF THE TOOLS WE WILL USE WILL ASSUME NORMAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are already familiar with standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z = \\frac{X-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "customer_sc = scaler.fit_transform(customer_features)\n",
    "customer_sc_df = pd.DataFrame(customer_features, columns=customer_features.columns)\n",
    "\n",
    "sc_stats = customer_features.describe().T\n",
    "sc_stats['skew'] = st.skew(customer_features)\n",
    "sc_stats['kurt'] = st.kurtosis(customer_features)\n",
    "display(stats)\n",
    "display(sc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_features.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_features[col], label=col)\n",
    "    plt.axvline(customer_features[col].mean(), c='red')\n",
    "    plt.axvline(customer_features[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_sc_df.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_features[col], label=col)\n",
    "    plt.axvline(customer_features[col].mean(), c='red')\n",
    "    plt.axvline(customer_features[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANY OF THE TOOLS WE WILL USE WILL ASSUME NORMAL DATA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deskew the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two common approaches to deskewing data:\n",
    "\n",
    "- the log transform\n",
    "- scaling by the Box-Cox test\n",
    "\n",
    "For purposes of comparison, we will keep both transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously looked at scaling data as a preprocessing step. Note that scaling of data will have no effect on its skewness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can verify this is via a test of skewness.\n",
    "\n",
    "To perform this test we can use the `scipy.stats.skewtest`.\n",
    "\n",
    "This function tests the null hypothesis that the skewness of the population that the sample was drawn from is the same as that of a corresponding normal distribution. Remember, a low p-value means reject the null hypothesis i.e the data is skewed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in customer_sc_df.columns:\n",
    "    original_col_skewtest = st.skewtest(customer_features[col])\n",
    "    scaled_col_skewtest = st.skewtest(customer_sc_df[col])\n",
    "    print(\"{}\\norig skew test: {} \\nscaled skew test: {}\\n\\n\".format(col, \n",
    "                                                                     original_col_skewtest,\n",
    "                                                                     scaled_col_skewtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deskew by taking the log of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\n",
    "\n",
    "We will then scale the data after deskewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_log_df = np.log(customer_features)\n",
    "\n",
    "scaler.fit(customer_log_df)\n",
    "customer_log_sc = scaler.transform(customer_log_df)\n",
    "customer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in customer_log_df.columns:\n",
    "    original_col_skewtest = st.skewtest(customer_features[col])\n",
    "    scaled_col_skewtest = st.skewtest(customer_sc_df[col])\n",
    "    original_log_col_skewtest = st.skewtest(customer_log_df[col])\n",
    "    scaled_log_col_skewtest = st.skewtest(customer_log_sc_df[col])\n",
    "    print(\"\"\"{}\n",
    "    orig:       {} \n",
    "    scaled:     {}\n",
    "    orig log:   {}\n",
    "    scaled log: {}\n",
    "    \n",
    "    \"\"\".format(col, \n",
    "               original_col_skewtest,\n",
    "               scaled_col_skewtest, \n",
    "               original_log_col_skewtest,\n",
    "               scaled_log_col_skewtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_sc_df.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_sc_df[col], label=col)\n",
    "    plt.axvline(customer_sc_df[col].mean(), c='red')\n",
    "    plt.axvline(customer_sc_df[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_log_sc_df.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_log_sc_df[col], label=col)\n",
    "    plt.axvline(customer_log_sc_df[col].mean(), c='red')\n",
    "    plt.axvline(customer_log_sc_df[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deskew by Box-Cox Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box cox test works by identifying the optimum power, $\\lambda$ to raise the data where\n",
    "\n",
    "$$\\mathbf{x_i}' = \\frac{\\mathbf{x_i}^\\lambda -1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation in Python is\n",
    "\n",
    "```\n",
    "y = (x**lmbda - 1) / lmbda, for lmbda > 0\n",
    "    log(x),                 for lmbda = 0\n",
    "```\n",
    "\n",
    "`boxcox` requires the input data to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_box_cox_df = pd.DataFrame()\n",
    "for col in customer_features.columns:\n",
    "    box_cox_trans = st.boxcox(customer_features[col])[0]\n",
    "    customer_box_cox_df[col] = pd.Series(box_cox_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(customer_box_cox_df)\n",
    "customer_box_cox_sc = scaler.transform(customer_box_cox_df)\n",
    "customer_box_cox_sc_df = pd.DataFrame(customer_box_cox_sc, columns=customer_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in customer_log_df.columns:\n",
    "    original_col_skewtest = st.skewtest(customer_features[col])\n",
    "    scaled_col_skewtest = st.skewtest(customer_sc_df[col])\n",
    "    original_log_col_skewtest = st.skewtest(customer_log_df[col])\n",
    "    scaled_log_col_skewtest = st.skewtest(customer_log_sc_df[col])\n",
    "    original_box_cox_col_skewtest = st.skewtest(customer_box_cox_df[col])\n",
    "    scaled_box_cox_col_skewtest = st.skewtest(customer_box_cox_sc_df[col])\n",
    "    print(\"\"\"{}\n",
    "    orig:           {} \n",
    "    scaled:         {}\n",
    "    orig log:       {}\n",
    "    scaled log:     {}\n",
    "    orig box-cox:   {}\n",
    "    scaled box-cox: {}\n",
    "    \n",
    "    \"\"\".format(col, \n",
    "               original_col_skewtest,\n",
    "               scaled_col_skewtest, \n",
    "               original_log_col_skewtest,\n",
    "               scaled_log_col_skewtest, \n",
    "               original_box_cox_col_skewtest,\n",
    "               scaled_box_cox_col_skewtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(customer_box_cox_sc_df.columns):\n",
    "    fig.add_subplot(231+i)\n",
    "    sns.distplot(customer_box_cox_sc_df[col], label=col)\n",
    "    plt.axvline(customer_box_cox_sc_df[col].mean(), c='red')\n",
    "    plt.axvline(customer_box_cox_sc_df[col].median(), c='black')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify outliers in the data, we will use what is [the Tukey Method](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/). \n",
    "\n",
    "This means that we will look for points that are more than 1.5 times the Inter-quartile range above the third quartile or below the first quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_outliers(dataframe, col, param=1.5):\n",
    "    Q1 = np.percentile(dataframe[col], 25)\n",
    "    Q3 = np.percentile(dataframe[col], 75)\n",
    "    tukey_window = param*(Q3-Q1)\n",
    "    less_than_Q1 = dataframe[col] < Q1 - tukey_window\n",
    "    greater_than_Q3 = dataframe[col] > Q3 + tukey_window\n",
    "    tukey_mask = (less_than_Q1 | greater_than_Q3)\n",
    "    return dataframe[tukey_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in customer_log_sc_df:\n",
    "    print(col, display_outliers(customer_log_sc_df, col).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we count the rows that show up as an outlier more than once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outliers = []\n",
    "for col in customer_log_sc_df:\n",
    "    outlier_df = display_outliers(customer_log_sc_df, col)\n",
    "    raw_outliers += list(outlier_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_count = Counter(raw_outliers)\n",
    "outliers = [k for k,v in outlier_count.items() if v > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_log_sc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(customer_box_cox_sc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for \"the bend\". This will tell you how many components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_component_loadings = pd.DataFrame(pca.components_, columns=customer_box_cox_sc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "pca_labels = ['PC ' + str(i+1) +' // ' + str(round(ratio,2)) for i, ratio in enumerate(explained_variance_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = principal_component_loadings.plot(kind='bar', figsize=(20,6), rot=0)\n",
    "ax.set_xticklabels(pca_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "pca.fit(customer_box_cox_sc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_pca_df = pd.DataFrame(pca.transform(customer_box_cox_sc_df), \n",
    "                               columns=['Dim 1', 'Dim 2'],\n",
    "                               index=customer_box_cox_sc_df.index)\n",
    "sample_pca_df = pd.DataFrame(pca.transform(sample), \n",
    "                             columns=['Dim 1', 'Dim 2'],\n",
    "                             index=sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.add_subplot(121)\n",
    "plt.title(\"Original Data\")\n",
    "sns.heatmap(sample, annot=True, cbar=False, square=True)\n",
    "fig.add_subplot(122)\n",
    "plt.title(\"PCA transformed Data\")\n",
    "sns.heatmap(sample_pca_df, annot=True, cbar=False, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.plot(kind='bar', figsize=(12,8))\n",
    "plt.title(\"Original Data\")\n",
    "_ = plt.xticks(range(5),['Sample 1','Sample 2','Sample 3', 'Sample 4','Sample 5'])\n",
    "\n",
    "sample_pca_df.plot(kind='bar', figsize=(12,8))\n",
    "plt.title(\"PCA transformed Data\")\n",
    "_ = plt.xticks(range(5),['Sample 1','Sample 2','Sample 3', 'Sample 4','Sample 5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
